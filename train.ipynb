{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "\n",
    "import math\n",
    "\n",
    "import mne\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utils.dataset import CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulled from Dr. Karpathy's minGPT implementation\n",
    "class GELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
    "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainDecoderBlock(nn.Module):\n",
    "    def __init__(self, k, input_dims=320, skip=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.skip = skip\n",
    "\n",
    "        self.conv1 = nn.Conv1d(input_dims, 320, kernel_size=3, dilation=2**((2*k)%5), padding=\"same\")\n",
    "        self.conv2 = nn.Conv1d(320, 320, kernel_size=3, dilation=2**((2*k+1)%5), padding=\"same\")\n",
    "        self.conv3 = nn.Conv1d(320, 640, kernel_size=3, dilation=2, padding=\"same\")\n",
    "\n",
    "        self.bnorm1 = nn.BatchNorm1d(320)\n",
    "        self.bnorm2 = nn.BatchNorm1d(320)\n",
    "\n",
    "        self.gelu = GELU()\n",
    "\n",
    "        # channel dim\n",
    "        self.glu = nn.GLU(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.bnorm1(output)\n",
    "        output = self.gelu(output)\n",
    "\n",
    "        if self.skip:\n",
    "            # channel dim res connection\n",
    "            output = output + x\n",
    "\n",
    "            skip = output\n",
    "\n",
    "        output = self.conv2(output)\n",
    "        output = self.bnorm2(output)\n",
    "        output = self.gelu(output)\n",
    "\n",
    "        if self.skip:\n",
    "            output = output + skip\n",
    "\n",
    "        output = self.conv3(output)\n",
    "        output = self.glu(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(2, 1, kernel_size=3, padding=\"same\")\n",
    "        self.conv2 = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "        self.bnorm1 = nn.BatchNorm1d(1)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x dims - batch_size, C, T\n",
    "\n",
    "        # pool across channel dim\n",
    "        avg_pool = torch.unsqueeze(torch.mean(x, dim=1), dim=1)\n",
    "        max_pool = torch.unsqueeze(torch.max(x, dim=1).values, dim=1)\n",
    "\n",
    "        mask = torch.cat((avg_pool, max_pool), dim=1)\n",
    "\n",
    "        mask = self.conv1(mask)\n",
    "        mask = self.bnorm1(mask)\n",
    "        mask = self.sigmoid(mask)\n",
    "\n",
    "        # broadcasting multiplication operation\n",
    "        output = mask * x\n",
    "\n",
    "        output = self.conv2(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainDecoder(nn.Module):\n",
    "    def __init__(self, input_channels, num_k, num_freq_bands):\n",
    "        super().__init__()\n",
    "\n",
    "        self.spatial_attention = SpatialAttention(input_channels, 270)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(270, 270, kernel_size=1)\n",
    "        self.subject_layer = nn.Conv1d(270, 270, kernel_size=1)\n",
    "\n",
    "        self.decoder_blocks = []\n",
    "\n",
    "        for i in range(num_k):\n",
    "            if i == 0:\n",
    "                self.decoder_blocks += [BrainDecoderBlock(i+1, 270, False)]\n",
    "            else:\n",
    "                self.decoder_blocks += [BrainDecoderBlock(i+1, 320, True)]\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList(self.decoder_blocks)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(320, 640, kernel_size=1)\n",
    "        self.final_conv = nn.Conv1d(640, num_freq_bands, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.spatial_attention(x)\n",
    "\n",
    "        output = self.conv1(output)\n",
    "        output = self.subject_layer(output)\n",
    "\n",
    "        for block in self.decoder_blocks:\n",
    "            output = block(output)\n",
    "        \n",
    "        output = self.conv2(output)\n",
    "        output = self.final_conv(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100, 3600])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = 61\n",
    "F = 100\n",
    "T = 3600\n",
    "\n",
    "brain_decoder = BrainDecoder(input_channels=C, num_k=5, num_freq_bands=F)\n",
    "\n",
    "# batch_size, C, T\n",
    "test_data = torch.randn((32, C, T))\n",
    "\n",
    "# expected output dims: batch_size, F, T\n",
    "output = brain_decoder(test_data)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "model = bundle.get_model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[39m=\u001b[39m CustomDataset(subject_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./data/umich/S01.mat\u001b[39;49m\u001b[39m\"\u001b[39;49m, audio_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./data/umich/audio/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/facebook/utils/dataset.py:57\u001b[0m, in \u001b[0;36mCustomDataset.__init__\u001b[0;34m(self, subject_path, audio_dir)\u001b[0m\n\u001b[1;32m     53\u001b[0m all_audio_data \u001b[39m=\u001b[39m all_audio_data[\u001b[39m0\u001b[39m][:all_audio_data\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maudio_sample_rate\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m)\u001b[39m*\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maudio_sample_rate\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m)]\n\u001b[1;32m     55\u001b[0m all_audio_data \u001b[39m=\u001b[39m all_audio_data\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maudio_sample_rate\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m brain_data \u001b[39m=\u001b[39m brain_data[:, :brain_data\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m]\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m1500\u001b[39m\u001b[39m*\u001b[39m\u001b[39m1500\u001b[39m]\u001b[39m.\u001b[39mview(brain_data\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1500\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBrain data shape: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(brain_data\u001b[39m.\u001b[39mshape))\n\u001b[1;32m     60\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mWaveform shape: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(all_audio_data\u001b[39m.\u001b[39mshape))\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset(subject_path=\"./data/umich/S01.mat\", audio_dir=\"./data/umich/audio/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
