{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "\n",
    "import math\n",
    "\n",
    "import mne\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.dataset import CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulled from Dr. Karpathy's minGPT implementation\n",
    "class GELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
    "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainDecoderBlock(nn.Module):\n",
    "    def __init__(self, k, input_dims=320, skip=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.skip = skip\n",
    "\n",
    "        self.conv1 = nn.Conv1d(input_dims, 320, kernel_size=3, dilation=2**((2*k)%5), padding=\"same\")\n",
    "        self.conv2 = nn.Conv1d(320, 320, kernel_size=3, dilation=2**((2*k+1)%5), padding=\"same\")\n",
    "        self.conv3 = nn.Conv1d(320, 640, kernel_size=3, dilation=2, padding=\"same\")\n",
    "\n",
    "        self.bnorm1 = nn.BatchNorm1d(320)\n",
    "        self.bnorm2 = nn.BatchNorm1d(320)\n",
    "\n",
    "        self.gelu = GELU()\n",
    "\n",
    "        # channel dim\n",
    "        self.glu = nn.GLU(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.bnorm1(output)\n",
    "        output = self.gelu(output)\n",
    "\n",
    "        if self.skip:\n",
    "            # channel dim res connection\n",
    "            output = output + x\n",
    "\n",
    "            skip = output\n",
    "\n",
    "        output = self.conv2(output)\n",
    "        output = self.bnorm2(output)\n",
    "        output = self.gelu(output)\n",
    "\n",
    "        if self.skip:\n",
    "            output = output + skip\n",
    "\n",
    "        output = self.conv3(output)\n",
    "        output = self.glu(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_harmonics, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # position preprocessing\n",
    "        easycap_montage = mne.channels.read_custom_montage(\"./data/umich/electrode_positions.sfp\")\n",
    "\n",
    "        info = mne.create_info([str(i+1) for i in range(in_channels)], sfreq=500, ch_types=\"eeg\")\n",
    "        info.set_montage(easycap_montage, on_missing=\"ignore\")\n",
    "\n",
    "        layout = mne.channels.find_layout(info)\n",
    "        two_dim_pos = layout.pos[:, :2]\n",
    "\n",
    "        # normalize 0-1\n",
    "        two_dim_pos[:, 0] -= min(two_dim_pos[:, 0])\n",
    "        two_dim_pos[:, 1] -= min(two_dim_pos[:, 1])\n",
    "\n",
    "        two_dim_pos[:, 0] /= max(two_dim_pos[:, 0])\n",
    "        two_dim_pos[:, 1] /= max(two_dim_pos[:, 1])\n",
    "\n",
    "        self.input_channels = torch.tensor(two_dim_pos)\n",
    "        \n",
    "        # spatial attention calculation params\n",
    "\n",
    "        self.z_trainable = torch.randn(out_channels, num_harmonics, num_harmonics, dtype=torch.cfloat)\n",
    "        self.z_trainable = torch.nn.parameter.Parameter(torch.transpose(self.z_trainable.view(1, out_channels, num_harmonics, num_harmonics).repeat(in_channels, 1, 1, 1), 0, 1))\n",
    "\n",
    "        self.k = self.l = torch.linspace(1, num_harmonics, num_harmonics).repeat(in_channels, num_harmonics, 1)\n",
    "\n",
    "        # other stuff\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def _apply(self, fn):\n",
    "        super(SpatialAttention, self)._apply(fn)\n",
    "\n",
    "        self.k = fn(self.k)\n",
    "        self.l = fn(self.l)\n",
    "        self.input_channels = fn(self.input_channels)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x dims - batch_size, C, T\n",
    "\n",
    "        term_1 = torch.transpose(torch.cos(2 * np.pi * (self.k * self.input_channels[:, 0].view(-1, 1, 1) + torch.transpose(self.l, 1, -1) * self.input_channels[:, 1].view(1, -1, 1, 1))), 2, 3)\n",
    "        term_2 = torch.transpose(torch.sin(2 * np.pi * (self.k * self.input_channels[:, 0].view(-1, 1, 1) + torch.transpose(self.l, 1, -1) * self.input_channels[:, 1].view(1, -1, 1, 1))), 2, 3)\n",
    "\n",
    "        a_j = torch.sum(self.z_trainable.real * term_1 + self.z_trainable.imag * term_2, dim=(1, 2, 3)).repeat(x.shape[0], 1).view(x.shape[0], -1, 1)\n",
    "\n",
    "        output = self.conv1(x)\n",
    "\n",
    "        output = self.dropout(self.softmax(a_j)) * output\n",
    "\n",
    "        return output.type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainDecoder(nn.Module):\n",
    "    def __init__(self, input_channels, num_k, num_freq_bands, num_subjects):\n",
    "        super().__init__()\n",
    "\n",
    "        self.spatial_attention = SpatialAttention(input_channels, 270, 32)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(270, 270, kernel_size=1)\n",
    "\n",
    "        self.subject_layers = []\n",
    "        for i in range(num_subjects):\n",
    "            self.subject_layers += [nn.Conv1d(270, 270, kernel_size=1)]\n",
    "        self.subject_layers = nn.ModuleList(self.subject_layers)\n",
    "\n",
    "        self.decoder_blocks = []\n",
    "\n",
    "        for i in range(num_k):\n",
    "            if i == 0:\n",
    "                self.decoder_blocks += [BrainDecoderBlock(i+1, 270, False)]\n",
    "            else:\n",
    "                self.decoder_blocks += [BrainDecoderBlock(i+1, 320, True)]\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList(self.decoder_blocks)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(320, 640, kernel_size=1)\n",
    "        self.final_conv = nn.Conv1d(640, num_freq_bands, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, subject_num):\n",
    "        output = self.spatial_attention(x)\n",
    "\n",
    "        output = self.conv1(output)\n",
    "\n",
    "        final_output = torch.zeros_like(output)\n",
    "\n",
    "        for _, i in enumerate(subject_num):\n",
    "            final_output[_] = self.subject_layers[int(i)](output[_])\n",
    "\n",
    "        for block in self.decoder_blocks:\n",
    "            final_output = block(final_output)\n",
    "        \n",
    "        final_output = self.conv2(final_output)\n",
    "        final_output = self.final_conv(final_output)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_subjects = 49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24364/126286730.py:8: RuntimeWarning: Fiducial point nasion not found, assuming identity unknown to head transformation\n",
      "  info.set_montage(easycap_montage, on_missing=\"ignore\")\n",
      "/tmp/ipykernel_24364/126286730.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = self.dropout(self.softmax(a_j)) * output\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 768, 149])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = 61\n",
    "F = 768\n",
    "T_out = 149\n",
    "\n",
    "brain_decoder = BrainDecoder(input_channels=C, num_k=5, num_freq_bands=F, num_subjects=num_subjects)\n",
    "brain_decoder = brain_decoder.to(device)\n",
    "\n",
    "# batch_size, C, T\n",
    "test_data = torch.randn((32, C, T_out))\n",
    "\n",
    "# expected output dims: batch_size, F, T_out\n",
    "output = brain_decoder(test_data.to(device), torch.zeros((32)))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "wave2vec = bundle.get_model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [01:25<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brain data shape: torch.Size([7953, 61, 149])\n",
      "Waveform shape: torch.Size([7953, 48000])\n",
      "Subject num shape: torch.Size([7953])\n",
      "Audio sampling rate (Hz): 16000\n",
      "Brain data sampling rate (Hz): 500\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "exclude = [2, 7, 9, 23, 24, 27, 28, 29, 30, 31, 32, 33, 43, 46, 47, 49]\n",
    "\n",
    "dataset = CustomDataset(data_dir=\"./data/umich\", T_out=T_out, num_subjects=num_subjects, exclude=exclude)\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [0.8, 0.2])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THEIR|FRIENDS|HAD|TAUGHT|THEM|SUCH|AS|THAT|\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(train_loader))\n",
    "\n",
    "with torch.inference_mode():\n",
    "    emission, _ = wave2vec(sample[1][0].view(1, -1).to(device).type(torch.float32))\n",
    "\n",
    "outputs = torch.argmax(emission[0], 1)\n",
    "outputs = torch.unique_consecutive(outputs, dim=0)\n",
    "\n",
    "labels = bundle.get_labels()\n",
    "\n",
    "output = \"\"\n",
    "\n",
    "for sample in outputs:\n",
    "    output += labels[sample]\n",
    "\n",
    "output = output.replace(\"-\", \"\")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "def CLIP(brain_latents, audio_latents, temperature):\n",
    "    # dims = batch_size, frequency_dim, temporal_dim\n",
    "    brain_latents = brain_latents.reshape((brain_latents.shape[0], -1)) # [batch_size, frequency_dim * temporal_dim]\n",
    "    # dims = batch_size, frequency_dim, temporal_dim\n",
    "    audio_latents = audio_latents.reshape((audio_latents.shape[0], -1)) # [batch_size, frequency_dim * temporal_dim]\n",
    "\n",
    "    logits = brain_latents @ audio_latents.T\n",
    "    logits_T = logits.T\n",
    "\n",
    "    labels = torch.arange(brain_latents.shape[0]).to(device)\n",
    "\n",
    "    loss = (cross_entropy(logits, labels) + cross_entropy(logits_T, labels)) / 2\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(brain_decoder.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]/tmp/ipykernel_24364/126286730.py:55: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = self.dropout(self.softmax(a_j)) * output\n",
      "100%|██████████| 50/50 [00:17<00:00,  2.84it/s]\n",
      "9it [00:02,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 10.58946647644043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:03<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 10.323194650503305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:17<00:00,  2.87it/s]\n",
      "9it [00:02,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 29.380656814575197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:03<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 25.362023500295784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:17<00:00,  2.86it/s]\n",
      "9it [00:02,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 20.362171745300294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:03<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 20.497876534095177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:17<00:00,  2.85it/s]\n",
      "9it [00:02,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 6.223719263076783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:03<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 6.462379822364221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:17<00:00,  2.85it/s]\n",
      "9it [00:02,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 5.898734521865845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:03<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.9638069959787225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:17<00:00,  2.85it/s]\n",
      "9it [00:02,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 5.421477317810059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:03<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.556553510519175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:17<00:00,  2.85it/s]\n",
      "9it [00:02,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 5.258290386199951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:03<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 5.478710211240328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:17<00:00,  2.85it/s]\n",
      "9it [00:02,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 5.8661253452301025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 9/13 [00:02<00:01,  3.42it/s]"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "N_loss = 10\n",
    "\n",
    "\n",
    "wave2vec.eval()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # training loop\n",
    "    brain_decoder.train()\n",
    "    for (brain_data, audio_data, subject_num) in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        brain_data = brain_data.to(device).type(torch.float32)\n",
    "        audio_data = audio_data.to(device).type(torch.float32)\n",
    "\n",
    "        # wave2vec processing\n",
    "        with torch.inference_mode():\n",
    "            features, _ = wave2vec.extract_features(audio_data)\n",
    "\n",
    "        # pull from 10th layer\n",
    "        semantic_features = features[9]     # dims -> batch_size, T, F\n",
    "        semantic_features = torch.transpose(semantic_features, 1, 2)\n",
    "\n",
    "        # brain decoder processing\n",
    "        brain_output = brain_decoder(brain_data, subject_num)\n",
    "\n",
    "        # propogate gradients\n",
    "        loss = CLIP(brain_output, semantic_features, 1.0)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    # set to eval mode\n",
    "    brain_decoder.eval()\n",
    "\n",
    "    # train loss calculation loop\n",
    "    train_losses = []\n",
    "    with torch.no_grad():\n",
    "        for i, (brain_data, audio_data, subject_num) in tqdm(enumerate(train_loader)):\n",
    "            brain_data = brain_data.to(device).type(torch.float32)\n",
    "            audio_data = audio_data.to(device).type(torch.float32)\n",
    "\n",
    "            # wave2vec processing\n",
    "            with torch.inference_mode():\n",
    "                features, _ = wave2vec.extract_features(audio_data)\n",
    "\n",
    "            # pull from 10th layer\n",
    "            semantic_features = features[9]     # dims -> batch_size, T, F\n",
    "            semantic_features = torch.transpose(semantic_features, 1, 2)\n",
    "\n",
    "            # brain decoder processing\n",
    "            brain_output = brain_decoder(brain_data, subject_num)\n",
    "\n",
    "            # propogate gradients\n",
    "            loss = CLIP(brain_output, semantic_features, 1.0)\n",
    "\n",
    "            train_losses += [loss.item()]\n",
    "\n",
    "            if i == N_loss-1:\n",
    "                print(\"Train loss: \" + str(np.mean(np.array(train_losses))))\n",
    "                break\n",
    "    \n",
    "    # val loss calculation loop\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for (brain_data, audio_data, subject_num) in tqdm(val_loader):\n",
    "            brain_data = brain_data.to(device).type(torch.float32)\n",
    "            audio_data = audio_data.to(device).type(torch.float32)\n",
    "\n",
    "            # wave2vec processing\n",
    "            with torch.inference_mode():\n",
    "                features, _ = wave2vec.extract_features(audio_data)\n",
    "\n",
    "            # pull from 10th layer\n",
    "            semantic_features = features[9]     # dims -> batch_size, T, F\n",
    "            semantic_features = torch.transpose(semantic_features, 1, 2)\n",
    "\n",
    "            # brain decoder processing\n",
    "            brain_output = brain_decoder(brain_data, subject_num)\n",
    "\n",
    "            # propogate gradients\n",
    "            loss = CLIP(brain_output, semantic_features, 1.0)\n",
    "\n",
    "            val_losses += [loss.item()]\n",
    "    \n",
    "    print(\"Val loss: \" + str(np.mean(np.array(val_losses))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
